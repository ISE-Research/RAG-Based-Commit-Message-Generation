{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876a6733-ad22-4b0b-aed7-62656059ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import faiss\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DPRQuestionEncoderTokenizerFast, T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import RagSequenceForGeneration, T5ForConditionalGeneration\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "import textstat\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea36754-99e6-4784-8bd9-76139e791c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries loaded: 500\n",
      "\n",
      "--- Dataset Inspection ---\n",
      "\n",
      "Total entries: 500\n",
      "\n",
      "\n",
      "No duplicate diff_ids found.\n",
      "\n",
      "Sample 3 entries:\n",
      "\n",
      "--- Entry 1 ---\n",
      "diff_id: 100\n",
      "Diff:\n",
      "mmm a / IPython / lib / irunner . py <nl> ppp b / IPython / lib / irunner . py <nl> <nl> import os <nl> import sys <nl> <nl> - # Third - party modules . <nl> - import pexpect <nl> + # Third - party mo...\n",
      "Message: Use IPython . external for pexpect import .\n",
      "...\n",
      "\n",
      "--- Entry 2 ---\n",
      "diff_id: 103\n",
      "Diff:\n",
      "mmm a / tornado / locks . py <nl> ppp b / tornado / locks . py <nl> <nl> __all__ = [ ' Condition ' , ' Event ' , ' Semaphore ' ] <nl> <nl> import collections <nl> - import contextlib <nl> <nl> from to...\n",
      "Message: Simpler code for Semaphore . acquire ( ) as a context manager .\n",
      "...\n",
      "\n",
      "--- Entry 3 ---\n",
      "diff_id: 105\n",
      "Diff:\n",
      "mmm a / Lib / test / test_resource . py <nl> ppp b / Lib / test / test_resource . py <nl> <nl> limit_set = 0 <nl> f = open ( TESTFN , \" wb \" ) <nl> f . write ( \" X \" * 1024 ) <nl> + f . flush ( ) <nl>...\n",
      "Message: Try harder to provoke the exception since the ia64 buildbot still\n",
      "...\n",
      "\n",
      "--- End of Inspection ---\n",
      "\n",
      "Total valid entries after lenient cleaning: 500\n",
      "Total excluded entries after lenient cleaning: 0\n",
      "Excluded entries saved to excluded_entries_lenient.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 700 entries\n",
      "Validation set: 150 entries\n",
      "Test set: 150 entries\n",
      "Serialized train set to prepared_data/train_data.pkl\n",
      "Serialized validation set to prepared_data/validation_data.pkl\n",
      "Serialized test set to prepared_data/test_data.pkl\n",
      "All data splits verified successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Data Loading, Cleaning, and Augmentation\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the JSONL dataset.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                dataset.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[Line {line_number}] JSON decoding error: {e}\")\n",
    "    return dataset\n",
    "\n",
    "def inspect_dataset(dataset, sample_size=5):\n",
    "    \"\"\"\n",
    "    Inspect the dataset for common issues.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Dataset Inspection ---\\n\")\n",
    "    print(f\"Total entries: {len(dataset)}\\n\")\n",
    "    \n",
    "    \n",
    "    missing_fields = defaultdict(int)\n",
    "    for entry in dataset:\n",
    "        for field in ['diff', 'msg']:\n",
    "            if field not in entry or not entry[field].strip():\n",
    "                missing_fields[field] += 1\n",
    "    \n",
    "    for field, count in missing_fields.items():\n",
    "        print(f\"Entries missing or empty '{field}': {count}\")\n",
    "    \n",
    "    \n",
    "    diff_id_counts = defaultdict(int)\n",
    "    for entry in dataset:\n",
    "        diff_id = entry.get('diff_id')\n",
    "        if diff_id is not None:\n",
    "            diff_id_counts[diff_id] += 1\n",
    "    \n",
    "    duplicates = {k: v for k, v in diff_id_counts.items() if v > 1}\n",
    "    if duplicates:\n",
    "        print(f\"\\nDuplicate diff_ids found: {len(duplicates)}\")\n",
    "        for k, v in list(duplicates.items())[:5]:  \n",
    "            print(f\"diff_id: {k}, count: {v}\")\n",
    "    else:\n",
    "        print(\"\\nNo duplicate diff_ids found.\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\nSample {sample_size} entries:\")\n",
    "    for i, entry in enumerate(dataset[:sample_size], 1):\n",
    "        print(f\"\\n--- Entry {i} ---\")\n",
    "        print(f\"diff_id: {entry.get('diff_id')}\")\n",
    "        print(f\"Diff:\\n{entry.get('diff')[:200]}...\")  \n",
    "        print(f\"Message: {entry.get('msg')[:100]}...\")\n",
    "    \n",
    "    print(\"\\n--- End of Inspection ---\\n\")\n",
    "\n",
    "def clean_and_validate_dataset_lenient(dataset):\n",
    "    \"\"\"\n",
    "    Clean and validate the dataset with lenient criteria.\n",
    "    \"\"\"\n",
    "    cleaned_dataset = []\n",
    "    excluded_entries = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        diff = entry.get('diff', '').strip()\n",
    "        msg = entry.get('msg', '').strip()\n",
    "        diff_id = entry.get('diff_id')\n",
    "        \n",
    "        exclusion_reasons = []\n",
    "        \n",
    "        \n",
    "        if not diff:\n",
    "            exclusion_reasons.append('Missing diff')\n",
    "        if not msg:\n",
    "            exclusion_reasons.append('Missing commit message')\n",
    "        \n",
    "        if exclusion_reasons:\n",
    "            excluded_entry = {\n",
    "                'diff_id': diff_id,\n",
    "                'diff': diff,\n",
    "                'msg': msg,\n",
    "                'reasons': exclusion_reasons\n",
    "            }\n",
    "            excluded_entries.append(excluded_entry)\n",
    "        else:\n",
    "            \n",
    "            entry['diff'] = diff.replace('<nl>', '\\n')\n",
    "            entry['msg'] = ' '.join(msg.replace('<nl>', '\\n').split())\n",
    "            cleaned_dataset.append(entry)\n",
    "    \n",
    "    print(f\"Total valid entries after lenient cleaning: {len(cleaned_dataset)}\")\n",
    "    print(f\"Total excluded entries after lenient cleaning: {len(excluded_entries)}\")\n",
    "    \n",
    "    \n",
    "    excluded_path = 'excluded_entries_lenient.pkl'\n",
    "    with open(excluded_path, 'wb') as f:\n",
    "        pickle.dump(excluded_entries, f)\n",
    "    print(f\"Excluded entries saved to {excluded_path}\")\n",
    "    \n",
    "    return cleaned_dataset, excluded_entries\n",
    "\n",
    "def augment_messages(messages: List[str], num_aug=1) -> List[str]:\n",
    "    \"\"\"\n",
    "    Augment commit messages by paraphrasing.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\", device=0 if torch.cuda.is_available() else -1)\n",
    "    for msg in messages:\n",
    "        for _ in range(num_aug):\n",
    "            paraphrased = paraphraser(msg, max_length=128, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
    "            augmented.append(paraphrased)\n",
    "    return augmented\n",
    "\n",
    "def split_dataset(diffs, messages, diff_ids, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    assert train_size + val_size + test_size == 1.0, \"Train, validation and test sizes must sum to 1.\"\n",
    "  \n",
    "    train_diffs, temp_diffs, train_msgs, temp_msgs, train_ids, temp_ids = train_test_split(\n",
    "        diffs, messages, diff_ids, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    val_ratio = val_size / (val_size + test_size)\n",
    "    val_diffs, test_diffs, val_msgs, test_msgs, val_ids, test_ids = train_test_split(\n",
    "        temp_diffs, temp_msgs, temp_ids, test_size=1 - val_ratio, random_state=random_state)\n",
    "    \n",
    "    print(f\"Training set: {len(train_diffs)} entries\")\n",
    "    print(f\"Validation set: {len(val_diffs)} entries\")\n",
    "    print(f\"Test set: {len(test_diffs)} entries\")\n",
    "    \n",
    "    return {\n",
    "        'train': {'diffs': train_diffs, 'messages': train_msgs, 'diff_ids': train_ids},\n",
    "        'validation': {'diffs': val_diffs, 'messages': val_msgs, 'diff_ids': val_ids},\n",
    "        'test': {'diffs': test_diffs, 'messages': test_msgs, 'diff_ids': test_ids},\n",
    "    }\n",
    "\n",
    "def serialize_splits(dataset_splits, output_dir='prepared_data'):\n",
    "    \"\"\"\n",
    "    Serialize the dataset splits into files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for split_name, data in dataset_splits.items():\n",
    "        file_path = os.path.join(output_dir, f\"{split_name}_data.pkl\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Serialized {split_name} set to {file_path}\")\n",
    "\n",
    "def verify_data_splits(dataset_splits, expected_total):\n",
    "    \"\"\"\n",
    "    Verify the integrity of the dataset splits.\n",
    "    \"\"\"\n",
    "    total = sum(len(data['diffs']) for data in dataset_splits.values())\n",
    "    if total != expected_total:\n",
    "        print(f\"Verification failed: Total entries {total} != Expected {expected_total}\")\n",
    "        return False\n",
    "  \n",
    "    for split_name, data in dataset_splits.items():\n",
    "        if len(data['diffs']) != len(data['messages']) or len(data['diffs']) != len(data['diff_ids']):\n",
    "            print(f\"Verification failed for split {split_name}: Inconsistent lengths.\")\n",
    "            return False\n",
    "  \n",
    "    print(\"All data splits verified successfully.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "dataset_path = 'py.jsonl'  \n",
    "raw_dataset = load_dataset(dataset_path)\n",
    "print(f\"Total entries loaded: {len(raw_dataset)}\")\n",
    "\n",
    "inspect_dataset(raw_dataset, sample_size=3)\n",
    "\n",
    "cleaned_dataset_lenient, excluded_entries_lenient = clean_and_validate_dataset_lenient(raw_dataset)\n",
    "\n",
    "diffs = [entry['diff'] for entry in cleaned_dataset_lenient]\n",
    "messages = [entry['msg'] for entry in cleaned_dataset_lenient]\n",
    "diff_ids = [entry['diff_id'] for entry in cleaned_dataset_lenient]\n",
    "\n",
    "\n",
    "augmented_messages = augment_messages(messages)\n",
    "augmented_diffs = diffs * 1  \n",
    "augmented_diff_ids = diff_ids * 1  \n",
    "\n",
    "\n",
    "diffs.extend(augmented_diffs)\n",
    "messages.extend(augmented_messages)\n",
    "diff_ids.extend(augmented_diff_ids)\n",
    "\n",
    "expected_total = len(diffs)\n",
    "\n",
    "dataset_splits = split_dataset(diffs, messages, diff_ids)\n",
    "\n",
    "serialize_splits(dataset_splits)\n",
    "\n",
    "verify_data_splits(dataset_splits, expected_total=expected_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42201ed9-dfc2-41c0-ba47-e1d0e2b02922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to GPU.\n",
      "Processed batch 1/32\n",
      "Processed batch 2/32\n",
      "Processed batch 3/32\n",
      "Processed batch 4/32\n",
      "Processed batch 5/32\n",
      "Processed batch 6/32\n",
      "Processed batch 7/32\n",
      "Processed batch 8/32\n",
      "Processed batch 9/32\n",
      "Processed batch 10/32\n",
      "Processed batch 11/32\n",
      "Processed batch 12/32\n",
      "Processed batch 13/32\n",
      "Processed batch 14/32\n",
      "Processed batch 15/32\n",
      "Processed batch 16/32\n",
      "Processed batch 17/32\n",
      "Processed batch 18/32\n",
      "Processed batch 19/32\n",
      "Processed batch 20/32\n",
      "Processed batch 21/32\n",
      "Processed batch 22/32\n",
      "Processed batch 23/32\n",
      "Processed batch 24/32\n",
      "Processed batch 25/32\n",
      "Processed batch 26/32\n",
      "Processed batch 27/32\n",
      "Processed batch 28/32\n",
      "Processed batch 29/32\n",
      "Processed batch 30/32\n",
      "Processed batch 31/32\n",
      "Processed batch 32/32\n",
      "Generated embeddings with shape: (1000, 768)\n",
      "FAISS index built with 1000 vectors.\n",
      "FAISS index saved to kb_index.faiss\n",
      "Metadata mapping saved to metadata_mapping.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b948da56bf1c4a8c8b52941aacff1344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base dataset saved to kb_dataset\n",
      "FAISS index loaded from kb_index.faiss with 1000 vectors.\n",
      "Metadata mapping loaded from metadata_mapping.pkl with 1000 entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Embedding Generation and FAISS Indexing\n",
    "\n",
    "\n",
    "def load_embedding_model(model_name: str = 'microsoft/codebert-base') -> (AutoTokenizer, AutoModel):\n",
    "    \"\"\"\n",
    "    Load the tokenizer and model for embedding generation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Hugging Face model name.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[AutoTokenizer, AutoModel]: The tokenizer and model instances.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to('cuda')\n",
    "        print(\"Model moved to GPU.\")\n",
    "    else:\n",
    "        print(\"GPU not available. Using CPU.\")\n",
    "    \n",
    "    model.eval()  \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "tokenizer_embed, model_embed = load_embedding_model()\n",
    "\n",
    "def generate_embeddings(diffs: List[str],\n",
    "                        tokenizer: AutoTokenizer,\n",
    "                        model: AutoModel,\n",
    "                        batch_size: int = 32,\n",
    "                        device: str = 'cuda' if torch.cuda.is_available() else 'cpu') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of code diffs.\n",
    "\n",
    "    Args:\n",
    "        diffs (List[str]): List of code diffs.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for the model.\n",
    "        model (AutoModel): Pre-trained model for embedding generation.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    num_batches = (len(diffs) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_diffs = diffs[i*batch_size : (i+1)*batch_size]\n",
    "        encoded_input = tokenizer(batch_diffs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        \n",
    "        token_embeddings = model_output.last_hidden_state  \n",
    "        attention_mask = encoded_input['attention_mask']  \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "embeddings = generate_embeddings(diffs, tokenizer_embed, model_embed)\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray, index_path: str = 'kb_index.faiss') -> faiss.IndexFlatIP:\n",
    "    \"\"\"\n",
    "    Build a FAISS index from the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of embeddings.\n",
    "        index_path (str): Path to save the FAISS index.\n",
    "\n",
    "    Returns:\n",
    "        faiss.IndexFlatIP: The FAISS index instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    \n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])  \n",
    "    \n",
    "    \n",
    "    index.add(embeddings)\n",
    "    print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "    \n",
    "    \n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"FAISS index saved to {index_path}\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "faiss_index = build_faiss_index(embeddings, index_path='kb_index.faiss')\n",
    "\n",
    "def create_metadata_mapping(diff_ids: List[int],\n",
    "                            diffs: List[str],\n",
    "                            messages: List[str],\n",
    "                            mapping_path: str = 'metadata_mapping.pkl') -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Create a metadata mapping linking FAISS index positions to their data.\n",
    "\n",
    "    Args:\n",
    "        diff_ids (List[int]): List of unique diff IDs.\n",
    "        diffs (List[str]): List of code diffs.\n",
    "        messages (List[str]): List of commit messages.\n",
    "        mapping_path (str): Path to save the metadata mapping.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Dict]: A dictionary mapping index positions to their data.\n",
    "    \"\"\"\n",
    "    assert len(diff_ids) == len(diffs) == len(messages), \"All input lists must have the same length.\"\n",
    "    \n",
    "    metadata_mapping = {}\n",
    "    for idx, (diff_id, diff, message) in enumerate(zip(diff_ids, diffs, messages)):\n",
    "        metadata_mapping[idx] = {\n",
    "            'diff_id': diff_id,\n",
    "            'diff': diff,\n",
    "            'message': message\n",
    "        }\n",
    "    \n",
    "    \n",
    "    with open(mapping_path, 'wb') as f:\n",
    "        pickle.dump(metadata_mapping, f)\n",
    "    print(f\"Metadata mapping saved to {mapping_path}\")\n",
    "    \n",
    "    return metadata_mapping\n",
    "\n",
    "\n",
    "metadata_mapping = create_metadata_mapping(diff_ids, diffs, messages)\n",
    "\n",
    "def create_knowledge_base(diff_ids: List[int], diffs: List[str], embeddings: np.ndarray, \n",
    "                         mapping: Dict[int, Dict], output_dir: str = 'kb_dataset') -> Dataset:\n",
    "    \"\"\"\n",
    "    Create and save the knowledge base dataset with required columns.\n",
    "\n",
    "    Args:\n",
    "        diff_ids (List[int]): List of unique diff IDs.\n",
    "        diffs (List[str]): List of code diffs.\n",
    "        embeddings (np.ndarray): Precomputed embeddings for each diff.\n",
    "        mapping (Dict[int, Dict]): Metadata mapping.\n",
    "        output_dir (str): Directory to save the knowledge base dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The knowledge base dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    titles = [f\"diff_{diff_id}\" for diff_id in diff_ids]  \n",
    "    texts = diffs\n",
    "    \n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    kb_dataset = Dataset.from_dict({\n",
    "        'title': titles,\n",
    "        'text': texts,\n",
    "        'embeddings': [embedding for embedding in embeddings]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    kb_dataset.save_to_disk(output_dir)\n",
    "    print(f\"Knowledge base dataset saved to {output_dir}\")\n",
    "    \n",
    "    return kb_dataset\n",
    "\n",
    "\n",
    "kb_dataset = create_knowledge_base(diff_ids, diffs, embeddings, metadata_mapping)\n",
    "\n",
    "def load_faiss_index(index_path: str = 'kb_index.faiss') -> faiss.IndexFlatIP:\n",
    "    \"\"\"\n",
    "    Load the FAISS index from the saved file.\n",
    "\n",
    "    Args:\n",
    "        index_path (str): Path to the FAISS index file.\n",
    "\n",
    "    Returns:\n",
    "        faiss.IndexFlatIP: The loaded FAISS index.\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(index_path)\n",
    "    print(f\"FAISS index loaded from {index_path} with {index.ntotal} vectors.\")\n",
    "    return index\n",
    "\n",
    "def load_metadata_mapping(mapping_path: str = 'metadata_mapping.pkl') -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Load the metadata mapping from the saved file.\n",
    "\n",
    "    Args:\n",
    "        mapping_path (str): Path to the metadata mapping file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Dict]: The loaded metadata mapping.\n",
    "    \"\"\"\n",
    "    with open(mapping_path, 'rb') as f:\n",
    "        metadata_mapping = pickle.load(f)\n",
    "    print(f\"Metadata mapping loaded from {mapping_path} with {len(metadata_mapping)} entries.\")\n",
    "    return metadata_mapping\n",
    "\n",
    "\n",
    "reloaded_faiss_index = load_faiss_index()\n",
    "reloaded_metadata_mapping = load_metadata_mapping()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2596c9bd-242d-4b57-b4a2-0509917a76c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d2c19efe764b7387e619e0349f1e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eadf814a414d8089bf61a8ec3a1562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a594aa2bd74455baa28ef9a2a61d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e982cb07b8549a7b0a50220a36a8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb811c150674f74b59bf1f50534e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-base were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddb810b363c4ddcac827cfd044411e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454df44d2f2e4c78aa12408e58767252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCS0289/myosc24/miniconda3/envs/rag/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3480' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3480/3480 2:01:54, Epoch 39/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>161.668400</td>\n",
       "      <td>158.467041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>102.638400</td>\n",
       "      <td>111.843674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>91.090300</td>\n",
       "      <td>103.949219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>86.872600</td>\n",
       "      <td>98.122093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>76.776100</td>\n",
       "      <td>93.833023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>64.372500</td>\n",
       "      <td>89.515648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>59.014100</td>\n",
       "      <td>86.209648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>49.087500</td>\n",
       "      <td>83.160515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>40.209400</td>\n",
       "      <td>81.268097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35.339700</td>\n",
       "      <td>79.636993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>29.529600</td>\n",
       "      <td>77.011696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>26.168500</td>\n",
       "      <td>75.892998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24.330700</td>\n",
       "      <td>74.842888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>18.952700</td>\n",
       "      <td>73.735107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>18.811500</td>\n",
       "      <td>72.764626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>14.501300</td>\n",
       "      <td>71.960426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>16.034600</td>\n",
       "      <td>71.458687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>14.144100</td>\n",
       "      <td>71.256905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>13.257900</td>\n",
       "      <td>71.126289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>11.262000</td>\n",
       "      <td>71.031982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>12.179200</td>\n",
       "      <td>70.978905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['rag.generator.model.encoder.embed_tokens.weight', 'rag.generator.model.decoder.embed_tokens.weight', 'generator.encoder.embed_tokens.weight', 'generator.decoder.embed_tokens.weight', 'generator.lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3480, training_loss=57.64258039189481, metrics={'train_runtime': 7316.8982, 'train_samples_per_second': 3.827, 'train_steps_per_second': 0.476, 'total_flos': 1.0211744425181184e+17, 'train_loss': 57.64258039189481, 'epoch': 39.77142857142857})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 3: Training the RAG Model with Joint Training and Hyperparameter Tuning\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def load_training_data(prepared_data_dir: str = 'prepared_data') -> Dict[str, List]:\n",
    "    dataset_splits = {}\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        file_path = os.path.join(prepared_data_dir, f\"{split}_data.pkl\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            split_data = pickle.load(f)\n",
    "        dataset_splits[split] = split_data\n",
    "    return dataset_splits\n",
    "\n",
    "dataset_splits = load_training_data()\n",
    "\n",
    "\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "generator_tokenizer = T5TokenizerFast.from_pretrained('t5-large')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_texts': dataset_splits['train']['diffs'],\n",
    "    'target_texts': dataset_splits['train']['messages']\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_texts': dataset_splits['validation']['diffs'],\n",
    "    'target_texts': dataset_splits['validation']['messages']\n",
    "})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    inputs = question_encoder_tokenizer(\n",
    "        examples['input_texts'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    labels = generator_tokenizer(\n",
    "        examples['target_texts'],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['input_texts', 'target_texts']\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['input_texts', 'target_texts']\n",
    ")\n",
    "\n",
    "\n",
    "kb_dataset_path = 'kb_dataset'  \n",
    "kb_index_path = 'kb_index.faiss'  \n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-sequence-base\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=kb_dataset_path,\n",
    "    index_path=kb_index_path,\n",
    "    use_dummy_dataset=False,\n",
    "    question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "    generator_tokenizer=generator_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(\n",
    "    \"facebook/rag-sequence-base\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "\n",
    "rag_model.generator = t5_model\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rag_model.to(device)\n",
    "\n",
    "\n",
    "for param in rag_model.question_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "rag_model.config.use_cache = False\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=generator_tokenizer,\n",
    "    model=rag_model,\n",
    "    label_pad_token_id=generator_tokenizer.pad_token_id,\n",
    "    padding='longest',\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./rag_model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=40,  \n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=rag_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c56641e-2957-43c3-9cfe-443a2bf48b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [06:30<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated messages saved to generated_messages.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/PCS0289/myosc24/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1078\n",
      "ROUGE1 Score: 0.3602\n",
      "ROUGE2 Score: 0.2336\n",
      "ROUGEL Score: 0.3340\n",
      "ROUGELSUM Score: 0.3327\n",
      "METEOR Score: 0.3697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.8401\n",
      "Accuracy: 0.00%\n",
      "Average Identifier Match Rate: 0.1297\n",
      "Average Readability (Flesch Reading Ease): 80.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Evaluating the Model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_test_dataset(prepared_data_dir='prepared_data') -> Dataset:\n",
    "    file_path = os.path.join(prepared_data_dir, \"test_data.pkl\")\n",
    "    with open(file_path, 'rb') as f:\n",
    "        split_data = pickle.load(f)\n",
    "    test_dataset = Dataset.from_dict({\n",
    "        'input_texts': split_data['diffs'],     \n",
    "        'target_texts': split_data['messages'], \n",
    "        'diff_ids': split_data['diff_ids']     \n",
    "    })\n",
    "    return test_dataset\n",
    "\n",
    "\n",
    "test_dataset = load_test_dataset()\n",
    "\n",
    "\n",
    "def generate_commit_messages(model, test_dataset, generator_tokenizer, device='cuda', output_file='generated_messages.jsonl'):\n",
    "    \"\"\"\n",
    "    Generate commit messages for the test dataset and save them in JSONL format.\n",
    "\n",
    "    Args:\n",
    "        model: The trained RAG model.\n",
    "        test_dataset: The test dataset.\n",
    "        generator_tokenizer: The generator tokenizer (T5TokenizerFast).\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        output_file (str): Path to save the generated messages.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    import gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for example in tqdm(test_dataset):\n",
    "            input_text = example['input_texts']\n",
    "            diff_id = example['diff_ids']\n",
    "\n",
    "            \n",
    "            inputs = model.retriever.question_encoder_tokenizer(\n",
    "                input_text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    num_beams=2,       \n",
    "                    max_length=64,     \n",
    "                    early_stopping=True,\n",
    "                    use_cache=False    \n",
    "                )\n",
    "\n",
    "            \n",
    "            generated_message = generator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            \n",
    "            json_obj = {\n",
    "                'diff_id': diff_id,\n",
    "                'diff': input_text,\n",
    "                'generated_message': generated_message\n",
    "            }\n",
    "\n",
    "            \n",
    "            f.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Generated messages saved to {output_file}\")\n",
    "\n",
    "\n",
    "generate_commit_messages(\n",
    "    model=rag_model,\n",
    "    test_dataset=test_dataset,\n",
    "    generator_tokenizer=generator_tokenizer,\n",
    "    device=device,\n",
    "    output_file='generated_messages.jsonl'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def load_generated_messages(generated_file='generated_messages.jsonl') -> List[Dict]:\n",
    "    generated = []\n",
    "    with open(generated_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            generated.append(data)\n",
    "    return generated\n",
    "\n",
    "\n",
    "def extract_messages(generated_data, test_dataset):\n",
    "    generated = []\n",
    "    references = []\n",
    "    \n",
    "    \n",
    "    ref_mapping = {entry['diff_ids']: entry['target_texts'] for entry in test_dataset}\n",
    "    \n",
    "    for entry in generated_data:\n",
    "        diff_id = entry['diff_id']\n",
    "        gen_msg = entry['generated_message']\n",
    "        ref_msg = ref_mapping.get(diff_id, \"\")\n",
    "        \n",
    "        if ref_msg:\n",
    "            generated.append(gen_msg)\n",
    "            references.append([ref_msg])  \n",
    "        else:\n",
    "            print(f\"Warning: No reference message found for diff_id {diff_id}\")\n",
    "    \n",
    "    return generated, references\n",
    "\n",
    "\n",
    "generated_data = load_generated_messages('generated_messages.jsonl')\n",
    "generated_messages, reference_messages = extract_messages(generated_data, test_dataset)\n",
    "\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "meteor = evaluate.load('meteor')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "\n",
    "\n",
    "def compute_bleu(generated: List[str], references: List[List[str]]):\n",
    "    results = bleu.compute(predictions=generated, references=references, smooth=True)\n",
    "    print(f\"BLEU Score: {results['bleu']:.4f}\")\n",
    "\n",
    "def compute_rouge_scores(generated: List[str], references: List[List[str]]):\n",
    "    results = rouge_metric.compute(predictions=generated, references=references, use_stemmer=True)\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key.upper()} Score: {value:.4f}\")\n",
    "\n",
    "def compute_meteor(generated: List[str], references: List[List[str]]):\n",
    "    results = meteor.compute(predictions=generated, references=references)\n",
    "    print(f\"METEOR Score: {results['meteor']:.4f}\")\n",
    "\n",
    "def compute_bertscore(generated: List[str], references: List[List[str]]):\n",
    "    results = bertscore.compute(\n",
    "        predictions=generated,\n",
    "        references=[ref[0] for ref in references],\n",
    "        lang='en'\n",
    "    )\n",
    "    f1 = np.mean(results['f1'])\n",
    "    print(f\"BERTScore F1: {f1:.4f}\")\n",
    "\n",
    "def compute_accuracy(generated: List[str], references: List[List[str]]):\n",
    "    correct = 0\n",
    "    total = len(generated)\n",
    "    for gen, ref in zip(generated, references):\n",
    "        if gen.strip().lower() == ref[0].strip().lower():\n",
    "            correct += 1\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "def compute_identifier_matches(generated: List[str], diffs: List[str]):\n",
    "    identifier_matches = []\n",
    "    for gen_msg, diff in zip(generated, diffs):\n",
    "       \n",
    "        identifiers = set([word.strip('`') for word in diff.split() if word.isidentifier()])\n",
    "        match_count = sum([1 for idf in identifiers if idf in gen_msg])\n",
    "        identifier_matches.append(match_count / len(identifiers) if identifiers else 0)\n",
    "    average_match = sum(identifier_matches) / len(identifier_matches)\n",
    "    print(f\"Average Identifier Match Rate: {average_match:.4f}\")\n",
    "    return average_match\n",
    "\n",
    "def compute_readability(generated: List[str]):\n",
    "    scores = [textstat.flesch_reading_ease(text) for text in generated]\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    print(f\"Average Readability (Flesch Reading Ease): {average_score:.2f}\")\n",
    "\n",
    "\n",
    "compute_bleu(generated_messages, reference_messages)\n",
    "compute_rouge_scores(generated_messages, reference_messages)\n",
    "compute_meteor(generated_messages, reference_messages)\n",
    "compute_bertscore(generated_messages, reference_messages)\n",
    "compute_accuracy(generated_messages, reference_messages)\n",
    "compute_identifier_matches(generated_messages, test_dataset['input_texts'])\n",
    "compute_readability(generated_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca1b92-303b-44ee-93ce-d5e59d44de47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
